{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data collection and preprocessing**"
      ],
      "metadata": {
        "id": "QnzsYbcx3FyK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKGi6Kge1akz"
      },
      "outputs": [],
      "source": [
        "# Data Collection and Preprocessing:\n",
        "# Collect and preprocess the text data, including cleaning, tokenization, normalization, and feature extraction.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.chunk import ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Sample text for NLP tasks\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language.\n",
        "NLP tasks include tokenization, part-of-speech tagging, named entity recognition, and much more.\n",
        "NLTK (Natural Language Toolkit) is a popular Python library for NLP tasks.\n",
        "\"\"\"\n",
        "# Load data\n",
        "#data = pd.read_csv('data.csv')\n",
        "\n",
        "# Preprocess text data\n",
        "# e.g., remove stopwords, punctuation, lowercase conversion, stemming, lemmization etc.\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokenized Text:\")\n",
        "print(tokens)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# Part-of-Speech (POS) Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "ner_tags = ne_chunk(pos_tags)\n",
        "print(\"\\nNER Tags:\")\n",
        "print(ner_tags)\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"\\nFiltered Tokens (without stopwords):\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "# Stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_tokens = [porter_stemmer.stem(word) for word in tokens]\n",
        "print(\"\\nStemmed Tokens:\")\n",
        "print(stemmed_tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"\\nLemmatized Tokens:\")\n",
        "print(lemmatized_tokens)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split data into traning  and testing sets**"
      ],
      "metadata": {
        "id": "owKOKMug3cYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the training data to a matrix of token counts\n",
        "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Initialize TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Transform the count matrix to a TF-IDF representation\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "metadata": {
        "id": "fQWGrpk53ER9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model selection and traning**"
      ],
      "metadata": {
        "id": "Q8jb2KEv4G3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection and Training:\n",
        "#Choose a suitable classification model and train it using the preprocessed data.\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "axMpfaWP4FRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation:**"
      ],
      "metadata": {
        "id": "P3Qn9cd34WDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the trained model on the test set using appropriate evaluation metrics.\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Transform the test data\n",
        "X_test_counts = count_vectorizer.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "VaW9nE6Y4OnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Deployment:**\n",
        "Deploy the trained model for inference on new text data."
      ],
      "metadata": {
        "id": "9iXFliAi4lqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(text):\n",
        "    # Preprocess the input text\n",
        "    text_counts = count_vectorizer.transform([text])\n",
        "    text_tfidf = tfidf_transformer.transform(text_counts)\n",
        "\n",
        "    # Make prediction\n",
        "    label = clf.predict(text_tfidf)\n",
        "\n",
        "    return label[0]\n",
        "\n",
        "# Example usage\n",
        "new_text = \"This is a test document.\"\n",
        "predicted_label = predict_label(new_text)\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "# Documentation and Reporting:\n",
        "# Document the project methodology, findings, and recommendations in a report or presentation.\n",
        "# This workflow covers the entire process of building and deploying a text classification model using scikit-learn in Python. Adjustments and additional steps may be needed based on the specific requirements and characteristics of your dataset\n",
        "# and task."
      ],
      "metadata": {
        "id": "ETFKBviL4gwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}